{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Blueprints for Text Analysis Using Python**](https://github.com/blueprints-for-text-analytics-python/blueprints-text)  \n",
    "Jens Albrecht, Sidharth Ramachandran, Christian Winkler\n",
    "\n",
    "**If you like the book or the code examples here, please leave a friendly comment on [Amazon.com](https://www.amazon.com/Blueprints-Text-Analytics-Using-Python/dp/149207408X)!**\n",
    "<img src=\"../rating.png\" width=\"100\"/>\n",
    "\n",
    "\n",
    "# Chapter 6:<div class='tocSkip'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use classification algorithms to label text into multiple categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark<div class='tocSkip'/>\n",
    "\n",
    "The code in this notebook differs slightly from the printed book. For example we frequently use pretty print (`pp.pprint`) instead of `print` and `tqdm`'s `progress_apply` instead of Pandas' `apply`. \n",
    "\n",
    "Moreover, several layout and formatting commands, like `figsize` to control figure size or subplot commands are removed in the book.\n",
    "\n",
    "You may also find some lines marked with three hashes ###. Those are not in the book as well as they don't contribute to the concept.\n",
    "\n",
    "All of this is done to simplify the code in the book and put the focus on the important parts instead of formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup<div class='tocSkip'/>\n",
    "\n",
    "Set directory locations. If working on Google Colab: copy files and install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    GIT_ROOT = 'https://github.com/blueprints-for-text-analytics-python/blueprints-text/raw/master'\n",
    "    os.system(f'wget {GIT_ROOT}/ch06/setup.py')\n",
    "\n",
    "%run -i setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Settings<div class=\"tocSkip\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"$BASE_DIR/settings.py\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)\n",
    "\n",
    "# path to import blueprints packages\n",
    "sys.path.append(BASE_DIR + '/packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import html \n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "## Depracated:\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "## New version:\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from blueprints.preparation import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll learn and what we will build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing the Java Development Tools Bug Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"eclipse_jdt.csv\"\n",
    "file = f\"{BASE_DIR}/data/jdt-bugs-dataset/eclipse_jdt.csv.gz\" ### real location\n",
    "df = pd.read_csv(file)\n",
    "print (df.columns)\n",
    "df[['Issue_id','Priority','Component','Title','Description']].sample(2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = df.drop(columns=['Duplicated_issue']) ###\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.sample(1, random_state=123).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Priority'].value_counts().sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Component'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprint: Building a Text Classification system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Title','Description','Priority']]\n",
    "df = df.dropna()\n",
    "df['text'] = df['Title'] + ' ' + df['Description']\n",
    "df = df.drop(columns=['Title','Description'])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blueprints.preparation import clean\n",
    "df['text'] = df['text'].apply(clean)\n",
    "df = df[df['text'].str.len() > 50]\n",
    "df.sample(2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
    "                                                    df['Priority'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['Priority'])\n",
    "\n",
    "print('Size of Training Data ', X_train.shape[0])\n",
    "print('Size of Test Data ', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Training the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 10, ngram_range=(1,2), stop_words=\"english\")\n",
    "X_train_tf = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LinearSVC(random_state=0, tol=1e-5)\n",
    "model1.fit(X_train_tf, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model1.predict(X_test_tf)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_baseline = clf.predict(X_test)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model1.predict(X_test_tf)\n",
    "confusion_matrix(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old code:\n",
    "\n",
    "# plot_confusion_matrix(model1,X_test_tf,\n",
    "#                       Y_test, values_format='d',\n",
    "#                       cmap=plt.cm.Blues)\n",
    "# plt.show()\n",
    "\n",
    "## New code:\n",
    "\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = model1.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter bug reports with priority P3 and sample 4000 rows from it\n",
    "df_sampleP3 = df[df['Priority'] == 'P3'].sample(n=4000, random_state=123)\n",
    "\n",
    "# Create a separate dataframe containing all other bug reports\n",
    "df_sampleRest = df[df['Priority'] != 'P3']\n",
    "\n",
    "# Concatenate the two dataframes to create the new balanced bug reports dataset\n",
    "df_balanced = pd.concat([df_sampleRest, df_sampleP3])\n",
    "\n",
    "# Check the status of the class imbalance\n",
    "df_balanced['Priority'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Blueprint for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the balanced dataframe\n",
    "\n",
    "df = df_balanced[['text', 'Priority']]\n",
    "df = df.dropna()\n",
    "\n",
    "# Step 1 - Data Preparation\n",
    "\n",
    "df['text'] = df['text'].apply(clean)\n",
    "\n",
    "# Step 2 - Train-Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
    "                                                    df['Priority'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['Priority'])\n",
    "print('Size of Training Data ', X_train.shape[0])\n",
    "print('Size of Test Data ', X_test.shape[0])\n",
    "\n",
    "# Step 3 - Training the Machine Learning model\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=10, ngram_range=(1, 2), stop_words=\"english\")\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "\n",
    "model1 = LinearSVC(random_state=0, tol=1e-5)\n",
    "model1.fit(X_train_tf, Y_train)\n",
    "\n",
    "# Step 4 - Model Evaluation\n",
    "\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "Y_pred = model1.predict(X_test_tf)\n",
    "print('Accuracy Score - ', accuracy_score(Y_test, Y_pred))\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyClassifier(strategy='stratified', random_state=21)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_baseline = clf.predict(X_test)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe combining the Title and Description, \n",
    "## Actual and Predicted values that we can explore\n",
    "frame = { 'text': X_test, 'actual': Y_test, 'predicted': Y_pred }\n",
    "result = pd.DataFrame(frame)\n",
    "\n",
    "result[((result['actual'] == 'P1') | (result['actual'] == 'P2')) &\n",
    "       (result['actual'] == result['predicted'])].sample(2, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[((result['actual'] == 'P1') | (result['actual'] == 'P2')) &\n",
    "       (result['actual'] != result['predicted'])].sample(2, random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df = 10, ngram_range=(1,2), stop_words=\"english\")\n",
    "df_tf = tfidf.fit_transform(df['text']).toarray()\n",
    "\n",
    "# Cross Validation with 5 folds\n",
    "\n",
    "scores = cross_val_score(estimator=model1,\n",
    "                         X=df_tf,\n",
    "                         y=df['Priority'],\n",
    "                         cv=5)\n",
    "\n",
    "print (\"Validation scores from each iteration of the cross validation \", scores)\n",
    "print (\"Mean value across of validation scores \", scores.mean())\n",
    "print (\"Standard deviation of validation scores \", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline = Pipeline(\n",
    "    steps=[('tfidf', TfidfVectorizer(\n",
    "        stop_words=\"english\")), ('model',\n",
    "                                 LinearSVC(random_state=21, tol=1e-5))])\n",
    "\n",
    "grid_param = [{\n",
    "    'tfidf__min_df': [5, 10],\n",
    "    'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
    "    'model__penalty': ['l2'],\n",
    "    'model__loss': ['hinge'],\n",
    "    'model__max_iter': [10000]\n",
    "}, {\n",
    "    'tfidf__min_df': [5, 10],\n",
    "    'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
    "    'model__C': [1, 10],\n",
    "    'model__tol': [1e-2, 1e-3]\n",
    "}]\n",
    "\n",
    "gridSearchProcessor = GridSearchCV(estimator=training_pipeline,\n",
    "                                   param_grid=grid_param,\n",
    "                                   cv=5)\n",
    "gridSearchProcessor.fit(df['text'], df['Priority'])\n",
    "\n",
    "best_params = gridSearchProcessor.best_params_\n",
    "print(\"Best alpha parameter identified by grid search \", best_params)\n",
    "\n",
    "best_result = gridSearchProcessor.best_score_\n",
    "print(\"Best result identified by grid search \", best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_results = pd.DataFrame(gridSearchProcessor.cv_results_)\n",
    "gridsearch_results[['rank_test_score', 'mean_test_score',\n",
    "                    'params']].sort_values(by=['rank_test_score'])[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blueprint recap and conclusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag that determines the choice of SVC (True) and LinearSVC (False)\n",
    "runSVC = True\n",
    "\n",
    "# Loading the dataframe\n",
    "\n",
    "file = \"eclipse_jdt.csv\"\n",
    "file = f\"{BASE_DIR}/data/jdt-bugs-dataset/eclipse_jdt.csv.gz\" ### real location\n",
    "df = pd.read_csv(file)\n",
    "df = df[['Title', 'Description', 'Component']]\n",
    "df = df.dropna()\n",
    "df['text'] = df['Title'] + df['Description']\n",
    "df = df.drop(columns=['Title', 'Description'])\n",
    "\n",
    "# Step 1 - Data Preparation\n",
    "df['text'] = df['text'].apply(clean)\n",
    "df = df[df['text'].str.len() > 50]\n",
    "\n",
    "if (runSVC):\n",
    "    # Sample the data when running SVC to ensure reasonable run-times\n",
    "    df = df.groupby('Component', as_index=False).apply(pd.DataFrame.sample,\n",
    "                                                       random_state=42,\n",
    "                                                       frac=.2)\n",
    "\n",
    "# Step 2 - Train-Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df['text'],\n",
    "                                                    df['Component'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=df['Component'])\n",
    "print('Size of Training Data ', X_train.shape[0])\n",
    "print('Size of Test Data ', X_test.shape[0])\n",
    "\n",
    "# Step 3 - Training the Machine Learning model\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "\n",
    "if (runSVC):\n",
    "    model = SVC(random_state=42, probability=True)\n",
    "    grid_param = [{\n",
    "        'tfidf__min_df': [5, 10],\n",
    "        'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
    "        'model__C': [1, 100],\n",
    "        'model__kernel': ['linear']\n",
    "    }]\n",
    "else:\n",
    "    model = LinearSVC(random_state=42, tol=1e-5)\n",
    "    grid_param = {\n",
    "        'tfidf__min_df': [5, 10],\n",
    "        'tfidf__ngram_range': [(1, 3), (1, 6)],\n",
    "        'model__C': [1, 100],\n",
    "        'model__loss': ['hinge']\n",
    "    }\n",
    "\n",
    "training_pipeline = Pipeline(\n",
    "    steps=[('tfidf', TfidfVectorizer(stop_words=\"english\")), ('model', model)])\n",
    "\n",
    "gridSearchProcessor = GridSearchCV(estimator=training_pipeline,\n",
    "                                   param_grid=grid_param,\n",
    "                                   cv=5)\n",
    "\n",
    "gridSearchProcessor.fit(X_train, Y_train)\n",
    "\n",
    "best_params = gridSearchProcessor.best_params_\n",
    "print(\"Best alpha parameter identified by grid search \", best_params)\n",
    "\n",
    "best_result = gridSearchProcessor.best_score_\n",
    "print(\"Best result identified by grid search \", best_result)\n",
    "\n",
    "best_model = gridSearchProcessor.best_estimator_\n",
    "\n",
    "# Step 4 - Model Evaluation\n",
    "\n",
    "Y_pred = best_model.predict(X_test)\n",
    "print('Accuracy Score - ', accuracy_score(Y_test, Y_pred))\n",
    "print(classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent', random_state=21)\n",
    "clf.fit(X_train, Y_train)\n",
    "Y_pred_baseline = clf.predict(X_test)\n",
    "print ('Accuracy Score - ', accuracy_score(Y_test, Y_pred_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe combining the Title and Description, \n",
    "## Actual and Predicted values that we can explore\n",
    "frame = { 'text': X_test, 'actual': Y_test, 'predicted': Y_pred } \n",
    "result = pd.DataFrame(frame)\n",
    "\n",
    "result[result['actual'] == result['predicted']].sample(2, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[result['actual'] != result['predicted']].sample(2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing Remarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blueprints",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
